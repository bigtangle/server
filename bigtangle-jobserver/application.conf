# Template for a Spark Job Server configuration file
# When deployed these settings are loaded when job server starts
#
# Spark Cluster / Job Server configuration

flyway.locations="db/mysql/migration"
     
  
spray.can.server {
      # Debug timeouts
      idle-timeout = infinite
      request-timeout = infinite
}
 
     
spark {
  # spark.master will be passed to each job's JobContext
  # local[...], yarn, mesos://... or spark://...
  master = "local[4]"

  # client or cluster deployment
  submit.deployMode = "client"

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 4 
  
  context-settings {
    # Dev debug timeout
    context-init-timeout = 1000000 s
  }

  jobserver {
    port = 8090
	#dev 
   context-creation-timeout = 1000000 s
    yarn-context-creation-timeout = 1000000 s
    default-sync-timeout = 1000000 s
    
    context-per-jvm = false

    sqldao {
      # Slick database driver, full classpath
  
          slick-driver = slick.driver.MySQLDriver

          # JDBC driver, full classpath
          jdbc-driver = com.mysql.jdbc.Driver

          jdbc {
            url = "jdbc:mysql://localhost:3306/spark_jobserver"
            user = "root"
            password = "test1234"
          }

          dbcp {
            maxactive = 20
            maxidle = 10
            initialsize = 10
          }
    }
    # When using chunked transfer encoding with scala Stream job results, this is the size of each chunk
    result-chunk-size = 1m
  }

    
  # Predefined Spark contexts
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #   }
  #   # define additional contexts here
  # }

  # Universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 2           # Number of cores to allocate.  Required.
    memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.

    # In case spark distribution should be accessed from HDFS (as opposed to being installed on every Mesos slave)
    # spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"

    # URIs of Jars to be loaded into the classpath for this context.
    # Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]

    # Add settings you wish to pass directly to the sparkConf as-is such as Hadoop connection
    # settings that don't use the "spark." prefix
    passthrough {
      #es.nodes = "192.1.1.1"
    }
  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  # home = "/home/spark/spark"
}

# Note that you can use this file to define settings not only for job server,
# but for your Spark jobs as well.  Spark job configuration merges with this configuration file as defaults.

akka {
  remote.netty.tcp {
    # This controls the maximum message size, including job results, that can be sent
    # maximum-frame-size = 10 MiB
  }
}
